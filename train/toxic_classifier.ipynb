{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import AdamW, AutoModel, AutoTokenizer, PreTrainedModel, PretrainedConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Import configuration variables\n",
    "from train_config import embedding_model_path, toxic_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters and paths\n",
    "DATASET_PATH = toxic_data_path\n",
    "EMBEDDING_MODEL_NAME = embedding_model_path\n",
    "SAVE_PATH = '../model/internal_classifier/sentiment_classifier'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEED = 42\n",
    "MAX_ITER = 20\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-5\n",
    "TRAIN_SIZE = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "df_train = pd.read_json(DATASET_PATH, lines=True)\n",
    "df_train = df_train[[\"text\", \"label\"]]\n",
    "\n",
    "# Balance the dataset by equal sampling from each class\n",
    "min_count = min(df_train['label'].value_counts())\n",
    "df_balanced = pd.concat([\n",
    "    df_train[df_train['label'] == 1].sample(n=min_count, random_state=SEED),\n",
    "    df_train[df_train['label'] == 0].sample(n=min_count, random_state=SEED)\n",
    "]).sample(frac=1, random_state=SEED)\n",
    "df_train = df_balanced\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "df_train, df_val = train_test_split(df_train, test_size=1 - TRAIN_SIZE, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class for PyTorch\n",
    "class CommentsDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for loading comments data.\"\"\"\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        encoded_input = self.tokenizer(\n",
    "            row['text'], \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoded_input['input_ids'].squeeze(),\n",
    "            'attention_mask': encoded_input['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(row['label'], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier model\n",
    "class GuideClassifier(PreTrainedModel):\n",
    "    \"\"\"Classifier model extending a base transformer model for sentiment analysis.\"\"\"\n",
    "    config_class = PretrainedConfig\n",
    "    \n",
    "    def __init__(self, base_model):\n",
    "        super(GuideClassifier, self).__init__(base_model.config)\n",
    "        self.sentence_transformer = base_model\n",
    "        self.classification_head = nn.Linear(base_model.config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        model_output = self.sentence_transformer(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        sentence_embeddings = self.mean_pooling(model_output, attention_mask)\n",
    "        logits = self.classification_head(sentence_embeddings)\n",
    "        return logits\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer, model, and datasets\n",
    "base_model_tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
    "base_model = AutoModel.from_pretrained(EMBEDDING_MODEL_NAME)\n",
    "train_dataset = CommentsDataset(df_train, base_model_tokenizer)\n",
    "val_dataset = CommentsDataset(df_val, base_model_tokenizer)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = GuideClassifier(base_model=base_model)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = BCEWithLogitsLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Prepare DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Training and validation loop\n",
    "train_losses, val_losses, val_accuracies = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(MAX_ITER):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_data in tqdm(train_loader, desc=f'Epoch {epoch+1}/{MAX_ITER}'):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=batch_data['input_ids'].to(DEVICE), attention_mask=batch_data['attention_mask'].to(DEVICE))\n",
    "        loss = criterion(outputs, batch_data['labels'].to(DEVICE).float().unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    total_eval_loss, total_eval_accuracy = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in tqdm(val_loader, desc='Validating'):\n",
    "            outputs = model(input_ids=batch_data['input_ids'].to(DEVICE), attention_mask=batch_data['attention_mask'].to(DEVICE))\n",
    "            loss = criterion(outputs, batch_data['labels'].to(DEVICE).float().unsqueeze(1))\n",
    "            total_eval_loss += loss.item()\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            total_eval_accuracy += accuracy_score(batch_data['labels'].cpu(), preds.cpu())\n",
    "\n",
    "    # Log and print epoch metrics\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    avg_val_loss = total_eval_loss / len(val_loader)\n",
    "    avg_val_accuracy = total_eval_accuracy / len(val_loader) * 100\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(avg_val_accuracy)\n",
    "    print(f'Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "def save_model(model, tokenizer, save_path: str):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    model_config = model.module.config if hasattr(model, 'module') else model.config\n",
    "    model_config.save_pretrained(save_path)\n",
    "    model_state_dict = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()\n",
    "    torch.save(model_state_dict, os.path.join(save_path, 'pytorch_model.bin'))\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "\n",
    "save_model(model, base_model_tokenizer, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation metrics\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_ctg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
